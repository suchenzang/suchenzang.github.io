# about me
i specialize in building big systems to crunch through big data and develop big models.

i have previously developed one of the largest reinforcement learning systems at openai for [openai five](https://web.archive.org/web/20230131202504/https://openai.com/five/), along with llm training infrastructure at meta ai / fair that created [opt-175b](https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/).

opt-175b was the first release in the industry to include:
* a 175b parameter model for research use
* a [114-page logbook](https://github.com/facebookresearch/metaseq/tree/main/projects/OPT/chronicles) detailing the challenges encountered during the 56 days it took to train a 175b llm on new hardware for the first time
* the entire training [codebase](https://github.com/facebookresearch/metaseq)
* a full suite of smaller-scale models ranging from [125M to 66B in size](https://github.com/facebookresearch/metaseq/tree/main/projects/OPT) for studying scaling laws.

before getting into ai systems, i worked on scaling out data infrastructure and processing pipelines across various cloud providers.

you can refer to my [linkedin](https://www.linkedin.com/in/suchenzang/) for more xp info.
# talks
in recent years, i have mainly presented talks on openai five and on opt-175b:

### openai-five
* Feb 12, 2022: Harvard CS50
  [![Harvard CS50](https://img.youtube.com/vi/J0KPNpro2J8/maxresdefault.jpg)](https://youtu.be/J0KPNpro2J8?t=6722)

* March 18, 2022: Computer History Museum
  [![Computer History Museum](https://img.youtube.com/vi/ej51D_Qd0N0/maxresdefault.jpg)](https://youtu.be/ej51D_Qd0N0?t=5735)

### opt-175b

* October 21, 2022: Scale Transform X Conference - [Top Tips from Netflix, NVIDIA, and Meta on Large Language Models](https://exchange.scale.com/public/videos/llms-tips-netflix-nvidia-meta-large-language-models) 

* December 2, 2022: NeurIPS 2022 - [Has It Trained Yet? Workshop](https://hity-workshop.github.io/NeurIPS2022/speakers/) 

* March 1, 2023: Stanford [MLSys](https://mlsys.stanford.edu/) Seminar Series
  [![Stanford MLSys](https://img.youtube.com/vi/p9IxoSkvZ-M/maxresdefault.jpg)](https://youtu.be/p9IxoSkvZ-M)
  
* April 1, 2023: CMU [LLM Seminar](https://cmu-lti-llm.org/talks/)

# publications
* [Scaling Autoregressive Multi-Modal Models:
Pretraining and Instruction Tuning](https://arxiv.org/abs/2309.02591)
* [LIMA: Less Is More for Alignment](https://arxiv.org/abs/2305.11206)
* [A Theory on Adam Instability in Large-Scale Machine Learning](https://arxiv.org/abs/2304.09871)
* [Effective Theory of Transformers at Initialization](https://arxiv.org/abs/2304.02034)
* [Scaling Laws for Generative Mixed-Modal Language Models](https://arxiv.org/abs/2301.03728)
* [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/pdf/2205.01068.pdf)
* [Neural Network Surgery with Sets](https://arxiv.org/pdf/1912.06719.pdf)
* [Long-Term Planning and Situational Awareness in OpenAI Five](https://arxiv.org/pdf/1912.06721.pdf) 
* [Dota 2 with Large Scale Deep Reinforcement Learning](https://arxiv.org/abs/1912.06680)
